{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a custom object detection model with Resnet FRCNN as backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-14 20:54:01.002263: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-14 20:54:01.112917: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-14 20:54:02.365777: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "import torch.nn as nn\n",
    "import torchvision.models.detection as detection\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "from torchvision.models.detection.roi_heads import RoIHeads\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "from torchvision.models.detection.faster_rcnn import TwoMLPHead, FastRCNNPredictor\n",
    "from torchvision.models.detection.faster_rcnn import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "%load_ext tensorboard\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "\n",
    "\n",
    "from LoadingBMWDataset import CustomObjectDetectionDataset\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Specify the image and annotation directories\n",
    "img_dir = '/home/wgt/Desktop/InMind Academy/AI_Track/Amazing_Project/inmind_amazing_project/data/Training/images'\n",
    "annotation_dir = '/home/wgt/Desktop/InMind Academy/AI_Track/Amazing_Project/inmind_amazing_project/data/Training/labels/json'\n",
    "\n",
    "# Create a CustomObjectDetectionDataset object\n",
    "dataset = CustomObjectDetectionDataset(img_dir, annotation_dir)\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(dataset_size * 0.8)\n",
    "validation_size = dataset_size - train_size\n",
    "\n",
    "train_dataset, validation_dataset = random_split(dataset, [train_size, validation_size])\n",
    "\n",
    "\n",
    "# The custom collate function will be used to stack the images and targets into a batch\n",
    "def collate_fn(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "\n",
    "    return images, targets\n",
    "\n",
    "# When creating the DataLoaders for training and validation, pass the custom collate function\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Ensring that the dataset is loaded correctly\n",
    "print(type(dataset[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgt/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/wgt/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Defining the custom model function to get the pre-trained Faster R-CNN model\n",
    "def get_custom_model(num_classes):\n",
    "    # Loading a pre-trained Faster R-CNN model\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "    # Getting the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # Creating the custom layers\n",
    "    # Note: It's crucial to match the input size of the first custom layer to 'in_features'\n",
    "    custom_head_layers = nn.Sequential(\n",
    "        nn.Linear(in_features, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n",
    "    # Extending the box predictor to include custom layers before final classification and regression layers\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(1024, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Using the function to get the customized model\n",
    "num_classes = 7 + 1  # 7 classes + background\n",
    "model = get_custom_model(num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Creating the optimizer based on the pre-trained model parameters and the modified custom layers\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "# Moving the model to the device\n",
    "model.to(device)\n",
    "\n",
    "writer = SummaryWriter('runs/faster-rcnn-object-detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No boxes found for image: 821.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgt/Desktop/InMind Academy/AI_Track/Amazing_Project/inmind_amazing_project/src/LoadingBMWDataset.py:79: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target['labels'] = torch.tensor(labels, dtype=torch.int64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No boxes found for image: 678.jpg\n",
      "No boxes found for image: 2433.jpg\n",
      "Epoch: 0, Loss: 0.10246841609477997\n",
      "No boxes found for image: 1154.jpg\n",
      "No boxes found for image: 821.jpg\n",
      "No boxes found for image: 2433.jpg\n",
      "No boxes found for image: 678.jpg\n",
      "Epoch: 1, Loss: 0.15769881010055542\n",
      "No boxes found for image: 1154.jpg\n",
      "No boxes found for image: 678.jpg\n",
      "No boxes found for image: 2433.jpg\n",
      "No boxes found for image: 821.jpg\n",
      "Epoch: 2, Loss: 0.11370489746332169\n",
      "No boxes found for image: 1154.jpg\n",
      "No boxes found for image: 2433.jpg\n",
      "No boxes found for image: 678.jpg\n",
      "No boxes found for image: 821.jpg\n",
      "Epoch: 3, Loss: 0.0877179205417633\n",
      "No boxes found for image: 1154.jpg\n",
      "No boxes found for image: 821.jpg\n",
      "No boxes found for image: 2433.jpg\n",
      "No boxes found for image: 678.jpg\n",
      "Epoch: 4, Loss: 0.11658086627721786\n",
      "No boxes found for image: 1154.jpg\n",
      "No boxes found for image: 2433.jpg\n",
      "No boxes found for image: 821.jpg\n",
      "No boxes found for image: 678.jpg\n",
      "Epoch: 5, Loss: 0.10396294295787811\n",
      "No boxes found for image: 1154.jpg\n",
      "No boxes found for image: 678.jpg\n",
      "No boxes found for image: 821.jpg\n",
      "No boxes found for image: 2433.jpg\n",
      "Epoch: 6, Loss: 0.07645800709724426\n",
      "No boxes found for image: 1154.jpg\n",
      "No boxes found for image: 678.jpg\n",
      "No boxes found for image: 2433.jpg\n",
      "No boxes found for image: 821.jpg\n",
      "Epoch: 7, Loss: 0.0797618180513382\n",
      "No boxes found for image: 1154.jpg\n",
      "No boxes found for image: 2433.jpg\n",
      "No boxes found for image: 821.jpg\n",
      "No boxes found for image: 678.jpg\n",
      "Epoch: 8, Loss: 0.08164059370756149\n",
      "No boxes found for image: 1154.jpg\n",
      "No boxes found for image: 678.jpg\n",
      "No boxes found for image: 821.jpg\n",
      "No boxes found for image: 2433.jpg\n",
      "Epoch: 9, Loss: 0.06159648299217224\n",
      "No boxes found for image: 1154.jpg\n",
      "No boxes found for image: 821.jpg\n",
      "No boxes found for image: 678.jpg\n",
      "No boxes found for image: 2433.jpg\n",
      "Epoch: 10, Loss: 0.06571391224861145\n",
      "No boxes found for image: 1154.jpg\n",
      "No boxes found for image: 2433.jpg\n",
      "No boxes found for image: 821.jpg\n",
      "No boxes found for image: 678.jpg\n",
      "Epoch: 11, Loss: 0.06687013059854507\n",
      "No boxes found for image: 1154.jpg\n",
      "No boxes found for image: 821.jpg\n",
      "No boxes found for image: 678.jpg\n",
      "No boxes found for image: 2433.jpg\n",
      "Epoch: 12, Loss: 0.07593970000743866\n",
      "No boxes found for image: 1154.jpg\n",
      "No boxes found for image: 821.jpg\n",
      "No boxes found for image: 678.jpg\n",
      "No boxes found for image: 2433.jpg\n",
      "Epoch: 13, Loss: 0.1135902851819992\n",
      "No boxes found for image: 1154.jpg\n",
      "No boxes found for image: 821.jpg\n",
      "No boxes found for image: 678.jpg\n",
      "No boxes found for image: 2433.jpg\n",
      "Epoch: 14, Loss: 0.07907244563102722\n",
      "No boxes found for image: 1154.jpg\n",
      "No boxes found for image: 821.jpg\n",
      "No boxes found for image: 2433.jpg\n",
      "No boxes found for image: 678.jpg\n",
      "Epoch: 15, Loss: 0.04091691970825195\n",
      "No boxes found for image: 1154.jpg\n",
      "No boxes found for image: 821.jpg\n",
      "No boxes found for image: 2433.jpg\n",
      "No boxes found for image: 678.jpg\n",
      "Epoch: 16, Loss: 0.06443195790052414\n",
      "No boxes found for image: 1154.jpg\n",
      "No boxes found for image: 2433.jpg\n",
      "No boxes found for image: 678.jpg\n",
      "No boxes found for image: 821.jpg\n",
      "Epoch: 17, Loss: 0.1312497854232788\n",
      "No boxes found for image: 1154.jpg\n",
      "No boxes found for image: 678.jpg\n",
      "No boxes found for image: 2433.jpg\n",
      "No boxes found for image: 821.jpg\n",
      "Epoch: 18, Loss: 0.049173563718795776\n",
      "No boxes found for image: 1154.jpg\n",
      "No boxes found for image: 2433.jpg\n",
      "No boxes found for image: 821.jpg\n",
      "No boxes found for image: 678.jpg\n",
      "Epoch: 19, Loss: 0.05583546683192253\n",
      "No boxes found for image: 1154.jpg\n",
      "No boxes found for image: 2433.jpg\n",
      "No boxes found for image: 678.jpg\n",
      "No boxes found for image: 821.jpg\n",
      "Epoch: 20, Loss: 0.04746192693710327\n",
      "No boxes found for image: 1154.jpg\n",
      "No boxes found for image: 821.jpg\n",
      "No boxes found for image: 678.jpg\n",
      "No boxes found for image: 2433.jpg\n",
      "Epoch: 21, Loss: 0.060477904975414276\n",
      "No boxes found for image: 1154.jpg\n",
      "No boxes found for image: 821.jpg\n",
      "No boxes found for image: 2433.jpg\n",
      "No boxes found for image: 678.jpg\n",
      "Epoch: 22, Loss: 0.08934785425662994\n",
      "No boxes found for image: 1154.jpg\n",
      "No boxes found for image: 821.jpg\n",
      "No boxes found for image: 2433.jpg\n",
      "No boxes found for image: 678.jpg\n",
      "Epoch: 23, Loss: 0.06865806877613068\n",
      "No boxes found for image: 1154.jpg\n",
      "No boxes found for image: 2433.jpg\n",
      "No boxes found for image: 678.jpg\n",
      "No boxes found for image: 821.jpg\n",
      "Epoch: 24, Loss: 0.05848095193505287\n",
      "No boxes found for image: 1154.jpg\n",
      "No boxes found for image: 678.jpg\n",
      "No boxes found for image: 821.jpg\n",
      "No boxes found for image: 2433.jpg\n",
      "Epoch: 25, Loss: 0.05322420597076416\n",
      "No boxes found for image: 1154.jpg\n",
      "No boxes found for image: 2433.jpg\n",
      "No boxes found for image: 678.jpg\n",
      "No boxes found for image: 821.jpg\n",
      "Epoch: 26, Loss: 0.08717910945415497\n",
      "No boxes found for image: 1154.jpg\n",
      "No boxes found for image: 2433.jpg\n",
      "No boxes found for image: 821.jpg\n",
      "No boxes found for image: 678.jpg\n",
      "Epoch: 27, Loss: 0.03672037273645401\n",
      "No boxes found for image: 1154.jpg\n",
      "No boxes found for image: 2433.jpg\n",
      "No boxes found for image: 678.jpg\n",
      "No boxes found for image: 821.jpg\n",
      "Epoch: 28, Loss: 0.04856476932764053\n",
      "No boxes found for image: 1154.jpg\n",
      "No boxes found for image: 821.jpg\n",
      "No boxes found for image: 678.jpg\n",
      "No boxes found for image: 2433.jpg\n",
      "Epoch: 29, Loss: 0.0349263921380043\n",
      "No boxes found for image: 1154.jpg\n",
      "No boxes found for image: 678.jpg\n",
      "No boxes found for image: 2433.jpg\n",
      "No boxes found for image: 821.jpg\n",
      "Epoch: 30, Loss: 0.041250817477703094\n",
      "No boxes found for image: 1154.jpg\n",
      "No boxes found for image: 678.jpg\n",
      "No boxes found for image: 2433.jpg\n",
      "No boxes found for image: 821.jpg\n",
      "Epoch: 31, Loss: 0.04784823954105377\n",
      "No boxes found for image: 1154.jpg\n",
      "No boxes found for image: 821.jpg\n",
      "No boxes found for image: 678.jpg\n",
      "No boxes found for image: 2433.jpg\n",
      "Epoch: 32, Loss: 0.0627702921628952\n",
      "No boxes found for image: 1154.jpg\n",
      "No boxes found for image: 821.jpg\n",
      "No boxes found for image: 678.jpg\n",
      "No boxes found for image: 2433.jpg\n",
      "Epoch: 33, Loss: 0.06776538491249084\n",
      "No boxes found for image: 1154.jpg\n",
      "No boxes found for image: 2433.jpg\n",
      "No boxes found for image: 678.jpg\n",
      "No boxes found for image: 821.jpg\n",
      "Epoch: 34, Loss: 0.03853088989853859\n",
      "No boxes found for image: 1154.jpg\n",
      "Epoch: 34, Training Loss: 0.0495, Validation Loss: 0.0963\n"
     ]
    }
   ],
   "source": [
    "# Selecting the parameters to be optimized\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "# Defining the optimizer, being the Stochastic Gradient Descent (SGD)\n",
    "optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 35\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    train_loss = 0\n",
    "    \n",
    "    # Training phase\n",
    "    for i, (images, targets) in enumerate(train_loader):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        train_loss += losses.item()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        writer.add_scalar('Loss/Train', losses.item(), epoch * len(train_loader) + i)\n",
    "\n",
    "    print(f'Epoch: {epoch}, Loss: {losses.item()}')\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    writer.add_scalar('Loss/train_avg', avg_train_loss, epoch)\n",
    "\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for images, targets in validation_loader:\n",
    "            images = list(img.to(device) for img in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # Temporarily set the model to training mode to compute losses\n",
    "            model.train()\n",
    "            loss_dict = model(images, targets)\n",
    "            model.eval()  # Set it back to evaluation mode\n",
    "            \n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            val_loss += losses.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(validation_loader)\n",
    "\n",
    "print(f'Epoch: {epoch}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model's parameters for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model's state dictionary\n",
    "torch.save(model.state_dict(), 'customResnet50FasterRCNN.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "# To reload the model with the last saved state dictionary (or parameters), run the following code:    #\n",
    "#                                                                                                      #\n",
    "#                                                                                                      #\n",
    "# Recreating the model instance to ensure the same architecture is used                                #\n",
    "model = get_custom_model(num_classes)\n",
    "\n",
    "# Loading the saved state dictionary into the model to pick up training from where we left off\n",
    "model.load_state_dict(torch.load('model_state_dict.pth'))\n",
    "\n",
    "# Setting the device to be used for training                                                           #\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "#                                                                                                      #\n",
    "# Move the model to the desired device                                                                 #\n",
    "model.to(device)\n",
    "########################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
