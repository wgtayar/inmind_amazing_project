{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LoadingBMWDataset import ObjectDetectionDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import glob\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import shutil\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_annotations_to_yolo_format(img_dir, annotation_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Ensure the output directory exists\n",
    "\n",
    "    # Iterating over each annotation file in the annotation directory, and convert the annotations to YOLO format\n",
    "    for annotation_file in os.listdir(annotation_dir):\n",
    "        if annotation_file.endswith('.json'):\n",
    "            json_path = os.path.join(annotation_dir, annotation_file)\n",
    "            with open(json_path) as f:\n",
    "                annotations = json.load(f)\n",
    "            \n",
    "            # Deriving the corresponding image file path and load it to get dimensions\n",
    "            img_file = annotation_file.replace('.json', '.jpg')\n",
    "            img_path = os.path.join(img_dir, img_file)\n",
    "            with Image.open(img_path) as img:\n",
    "                img_width, img_height = img.size\n",
    "            \n",
    "            # Converting annotations to YOLO format\n",
    "            yolo_annotations = []\n",
    "            for annot in annotations:\n",
    "                class_id = annot['ObjectClassId'] - 1  # Assuming class IDs start from 1, adjust if necessary\n",
    "                x_center = ((annot['Right'] + annot['Left']) / 2) / img_width\n",
    "                y_center = ((annot['Bottom'] + annot['Top']) / 2) / img_height\n",
    "                width = (annot['Right'] - annot['Left']) / img_width\n",
    "                height = (annot['Bottom'] - annot['Top']) / img_height\n",
    "                yolo_annotations.append(f\"{class_id} {x_center} {y_center} {width} {height}\")\n",
    "            \n",
    "            # Saving converted annotations to TXT file\n",
    "            txt_path = os.path.join(output_dir, annotation_file.replace('.json', '.txt'))\n",
    "            with open(txt_path, 'w') as f:\n",
    "                f.write('\\n'.join(yolo_annotations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = '/home/wgt/Desktop/InMind Academy/AI_Track/Amazing_Project/inmind_amazing_project/data/Training/images'\n",
    "annotation_dir = '/home/wgt/Desktop/InMind Academy/AI_Track/Amazing_Project/inmind_amazing_project/data/Training/labels/json'\n",
    "output_dir = '/home/wgt/Desktop/InMind Academy/AI_Track/Amazing_Project/inmind_amazing_project/data/Training/labels/yolo'\n",
    "\n",
    "convert_annotations_to_yolo_format(img_dir, annotation_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "\n",
    "annotations_txt_dir = '/home/wgt/Desktop/InMind Academy/AI_Track/Amazing_Project/inmind_amazing_project/data/Training/labels/yolo'\n",
    "img_dir = '/home/wgt/Desktop/InMind Academy/AI_Track/Amazing_Project/inmind_amazing_project/data/Training/images'\n",
    "\n",
    "# Listing all .txt annotation files\n",
    "annotation_files = glob.glob(os.path.join(annotations_txt_dir, '*.txt'))\n",
    "\n",
    "# Extracting corresponding image file names from annotation file names\n",
    "img_files = [os.path.join(img_dir, os.path.basename(f).replace('.txt', '.jpg')) for f in annotation_files]\n",
    "\n",
    "# Splitting the dataset into training and validation\n",
    "train_img_files, val_img_files, train_annotation_files, val_annotation_files = train_test_split(img_files, annotation_files, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 1995\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training images: {len(train_img_files)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Moving to YOLOV7 </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolov7_train_images_dir = '/home/wgt/yolov7/train/images'\n",
    "yolov7_train_labels_dir = '/home/wgt/yolov7/train/labels'\n",
    "\n",
    "os.makedirs(yolov7_train_images_dir, exist_ok=True)\n",
    "os.makedirs(yolov7_train_labels_dir, exist_ok=True)\n",
    "\n",
    "for img_file in train_img_files:\n",
    "    dest_file = os.path.join(yolov7_train_images_dir, os.path.basename(img_file))\n",
    "    shutil.copy(img_file, dest_file)\n",
    "    print(f\"Copied {img_file} to {dest_file}\")\n",
    "\n",
    "for label_file in train_annotation_files:\n",
    "    dest_file = os.path.join(yolov7_train_labels_dir, os.path.basename(label_file))\n",
    "    shutil.copy(label_file, dest_file)\n",
    "    print(f\"Copied {label_file} to {dest_file}\")\n",
    "\n",
    "yolov7_val_images_dir = '/home/wgt/yolov7/val/images'\n",
    "yolov7_val_labels_dir = '/home/wgt/yolov7/val/labels'\n",
    "\n",
    "os.makedirs(yolov7_val_images_dir, exist_ok=True)\n",
    "os.makedirs(yolov7_val_labels_dir, exist_ok=True)\n",
    "\n",
    "for img_file in val_img_files:\n",
    "    dest_file = os.path.join(yolov7_val_images_dir, os.path.basename(img_file))\n",
    "    shutil.copy(img_file, dest_file)\n",
    "    print(f\"Copied {img_file} to {dest_file}\")\n",
    "\n",
    "for label_file in val_annotation_files:\n",
    "    dest_file = os.path.join(yolov7_val_labels_dir, os.path.basename(label_file))\n",
    "    shutil.copy(label_file, dest_file)\n",
    "    print(f\"Copied {label_file} to {dest_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Results of the first Training process </center>\n",
    "\n",
    "| Class      | Images | Labels   | P       | R         | mAP@.5    | mAP@.5:.95|\n",
    "|------------|--------|----------|---------|-----------|-----------|-----------|\n",
    "| all        | 483    | 1995     | 0.96    | 0.954     | 0.976     | 0.919     |\n",
    "| dolly      | 483    | 847      | 0.934   | 0.913     | 0.957     | 0.855     |\n",
    "| bin        | 483    | 349      | 0.961   | 0.983     | 0.992     | 0.948     |             \n",
    "| jack       | 483    | 799      | 0.986   | 0.967     | 0.98      | 0.952     |\n",
    "\n",
    "<br> We can see that the precision is very high (96% overall), as well as the recall (95.4%). \n",
    "<br> The Mean Average Precision (AP) at IoU (Intersection over Union) threshold of 0.5 indicates a 97.6% meaning a very high accuracy.\n",
    "<br> The the mean AP calculated at different IoU thresholds from 0.5 to 0.95 (with a step size of 0.05), provides a more comprehensive view of the model performance across various strictness levels of object detection.\n",
    "\n",
    "<br> The model shows high precision and recall across the classes dolly, bin, and jack, with overall great mAP scores, which indicates strong performance in both recognizing the correct objects (high recall) and ensuring that most detections are accurate (high precision)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Preparing the Testing Dataset </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_dir = '/home/wgt/Desktop/InMind Academy/AI_Track/Amazing_Project/inmind_amazing_project/data/Testing/images'\n",
    "test_annotation_dir = '/home/wgt/Desktop/InMind Academy/AI_Track/Amazing_Project/inmind_amazing_project/data/Testing/labels/json'\n",
    "test_output_dir = '/home/wgt/Desktop/InMind Academy/AI_Track/Amazing_Project/inmind_amazing_project/data/Testing/labels/yolo'\n",
    "\n",
    "convert_annotations_to_yolo_format(test_img_dir, test_annotation_dir, test_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of testing images: 958\n",
      "Number of testing labels: 958\n",
      "Copied all testing images and labels to the YOLOv7 testing directory.\n"
     ]
    }
   ],
   "source": [
    "test_img_files = glob.glob(os.path.join(test_img_dir, '*.jpg'))\n",
    "test_annotation_files = [os.path.join(test_output_dir, os.path.basename(img_file).replace('.jpg', '.txt')) for img_file in test_img_files]\n",
    "\n",
    "yolov7_test_images_dir = '/home/wgt/yolov7/test/images'\n",
    "yolov7_test_labels_dir = '/home/wgt/yolov7/test/labels'\n",
    "\n",
    "print(f\"Number of testing images: {len(test_img_files)}\")\n",
    "print(f\"Number of testing labels: {len(test_annotation_files)}\")\n",
    "\n",
    "os.makedirs(yolov7_test_images_dir, exist_ok=True)\n",
    "os.makedirs(yolov7_test_labels_dir, exist_ok=True)\n",
    "\n",
    "for img_file in test_img_files:\n",
    "    dest_file = os.path.join(yolov7_test_images_dir, os.path.basename(img_file))\n",
    "    shutil.copy(img_file, dest_file)\n",
    "    # print(f\"Copied {img_file} to {dest_file}\")\n",
    "\n",
    "for label_file in test_annotation_files:\n",
    "    dest_file = os.path.join(yolov7_test_labels_dir, os.path.basename(label_file))\n",
    "    shutil.copy(label_file, dest_file)\n",
    "    # print(f\"Copied {label_file} to {dest_file}\")\n",
    "\n",
    "print('Copied all testing images and labels to the YOLOv7 testing directory.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Metrics after testing the YOLOV7 model the first time </center>\n",
    "\n",
    "### _*Hyperparameter Values*_\n",
    "\n",
    "### <font color=\"green\" size=5>\n",
    "- lr0: 0.01\n",
    "- lrf: 0.1\n",
    "- iou_t: 0.2\n",
    "- batches of 4\n",
    "- 50\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(file_path):\n",
    "    # Loading labels\n",
    "    with open(file_path, 'r') as f:\n",
    "        labels = [list(map(float, line.split()[1:])) for line in f.readlines()]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(box1, box2):\n",
    "\n",
    "    #Calculating the Intersection over Union (IoU) of two bounding boxes.\n",
    "    \n",
    "    # Converting from center coordinates to box corners\n",
    "    b1_x1, b1_y1, b1_x2, b1_y2 = box1[0] - box1[2] / 2, box1[1] - box1[3] / 2, box1[0] + box1[2] / 2, box1[1] + box1[3] / 2\n",
    "    b2_x1, b2_y1, b2_x2, b2_y2 = box2[0] - box2[2] / 2, box2[1] - box2[3] / 2, box2[0] + box2[2] / 2, box2[1] + box2[3] / 2\n",
    "\n",
    "    # Calculating intersection\n",
    "    inter = (max(0, min(b1_x2, b2_x2) - max(b1_x1, b2_x1)) *\n",
    "             max(0, min(b1_y2, b2_y2) - max(b1_y1, b2_y1)))\n",
    "    # Calculating union\n",
    "    union = (b1_x2 - b1_x1) * (b1_y2 - b1_y1) + (b2_x2 - b2_x1) * (b2_y2 - b2_y1) - inter\n",
    "\n",
    "    # Calculating IoU\n",
    "    iou = inter / union\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(predictions_dir, ground_truths_dir):\n",
    "    \n",
    "    # Evaluating the model by comparing predicted labels to ground truth labels\n",
    "    \n",
    "    # Loading all ground truth and prediction files\n",
    "    gt_files = glob.glob(os.path.join(ground_truths_dir, '*.txt'))\n",
    "    pred_files = glob.glob(os.path.join(predictions_dir, '*.txt'))\n",
    "\n",
    "    gt_labels = {os.path.basename(f): load_labels(f) for f in gt_files}\n",
    "    pred_labels = {os.path.basename(f): load_labels(f) for f in pred_files}\n",
    "\n",
    "    # Initializing variables to calculate precision and recall\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    iou_threshold = 0.6  # Defining the IoU threshold to consider a detection as a true positive\n",
    "\n",
    "    # Iterating over each ground truth file and comparing with the corresponding prediction file\n",
    "    for file_name, gt_boxes in gt_labels.items():\n",
    "        pred_boxes = pred_labels.get(file_name, [])\n",
    "        \n",
    "        matched = set()\n",
    "        for i, pred_box in enumerate(pred_boxes):\n",
    "            for j, gt_box in enumerate(gt_boxes):\n",
    "                if calculate_iou(pred_box, gt_box) >= iou_threshold and j not in matched:\n",
    "                    true_positives += 1\n",
    "                    matched.add(j)\n",
    "                    break\n",
    "            else:\n",
    "                false_positives += 1\n",
    "                \n",
    "        false_negatives += len(gt_boxes) - len(matched)\n",
    "\n",
    "    # Calculating precision and recall\n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5391\n",
      "Recall: 0.7352\n",
      "F1 Score: 0.6221\n"
     ]
    }
   ],
   "source": [
    "# Placeholder paths\n",
    "predictions_dir = '/home/wgt/yolov7/runs/detect/exp/labels'\n",
    "ground_truths_dir = '/home/wgt/yolov7/test/labels'\n",
    "\n",
    "evaluate_model(predictions_dir, ground_truths_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _**Analysis of the results**_\n",
    "<br> Precision: _0.5391_. A low precision value such as the one we got may indicate a relatively higher number of false positives.\n",
    "\n",
    "<br> Recall: _0.7352_. The model performed better in terms of recall. Higher recall means the model is better at detecting the objects of interest but may also be picking up some false positives to increase this score.\n",
    "\n",
    "<br> F1 score: _0.6221_. This indicates the model has a moderate level of performance. As the F1 score ranges between 0 and 1, a score of _0.6221_ ranks in the mid range. While there is room for improvement, this score does outline a positive, related to the balance of the model between precision and recall.\n",
    "\n",
    "<br> It is noteworthy to mention that precision and recall are inversely proprotional. However, it is still possible to improve both the values we got from the testing.\n",
    "\n",
    "<br> Here are some steps we can take to improve model performance:\n",
    "<br> \n",
    "1. Data Augmentation: Enhance the dataset with more varied data through augmentation techniques (e.g., flipping, scaling, cropping) to improve the model’s ability to generalize. This was already done in the DataAugmentation.py file.\n",
    "\n",
    "2. Hyperparameter Tuning: Adjusting learning rate, IoU, batch size, or other training parameters may help optimize training.\n",
    "\n",
    "3. Review of Misclassified Examples: Analyze specific cases where the model failed (both false positives and false negatives) to understand potential patterns or characteristics that led to incorrect predictions.\n",
    "\n",
    "4. Fine-Tuning: Since we started with pre-trained weights, further fine-tuning on our specific dataset could lead to improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _**Implementation of a mock mAP function**_\n",
    "<font color=\"red\" size=5> <br> For correct results, this function needs to have the per-class precision and recall curves. </br>\n",
    "<br> In this code, we will be using dummy values for the sake of demonstration.</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ap(recalls, precisions):\n",
    "\n",
    "    # Calculating the Average Precision (AP) based on recall and precision curves\n",
    "\n",
    "    # Adding end points to recall and precision arrays\n",
    "    recalls = np.concatenate(([0.], recalls, [1.]))\n",
    "    precisions = np.concatenate(([0.], precisions, [0.]))\n",
    "\n",
    "    # Calculating the precision envelope\n",
    "    for i in range(precisions.size - 1, 0, -1):\n",
    "        precisions[i - 1] = np.maximum(precisions[i - 1], precisions[i])\n",
    "\n",
    "    # Calculating AP\n",
    "    indices = np.where(recalls[1:] != recalls[:-1])[0]\n",
    "    ap = np.sum((recalls[indices + 1] - recalls[indices]) * precisions[indices + 1])\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_map(predictions, ground_truths, iou_threshold=0.5):\n",
    "    '''\n",
    "    Simplified mAP calculation at a single IoU threshold.\n",
    "    predictions: Dict of image_id to predicted bounding boxes [confidence_score, x, y, w, h]\n",
    "    ground_truths: Dict of image_id to ground truth bounding boxes [x, y, w, h]\n",
    "    '''\n",
    "    \n",
    "    # Mock data illustrating AP calculation\n",
    "    precisions = np.array([0.9, 0.75, 0.6])\n",
    "    recalls = np.array([0.5, 0.65, 0.85])\n",
    "    \n",
    "    ap = calculate_ap(recalls, precisions)\n",
    "    print(f\"AP: {ap:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP: 0.6825\n"
     ]
    }
   ],
   "source": [
    "# Mock function calls - Need to be replaced with actual data processing and mAP calculation\n",
    "predictions_dir = '/home/wgt/yolov7/runs/detect/exp/labels'\n",
    "ground_truths_dir = '/home/wgt/yolov7/test/labels'\n",
    "calculate_map(predictions_dir, ground_truths_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> <font color=\"white\"> Metrics after testing the YOLOV7 model the second time</font> </center>\n",
    "\n",
    "### _*New Hyperparameter Values*_\n",
    "\n",
    "### <font color=\"green\" size=5>\n",
    "- lr0: 0.0001\n",
    "- lrf: 0.01\n",
    "- iou_t: 0.6\n",
    "- batches of 4\n",
    "- 80 \n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5411\n",
      "Recall: 0.6790\n",
      "F1 Score: 0.6022\n"
     ]
    }
   ],
   "source": [
    "predictions_dir = '/home/wgt/yolov7/runs/detect/exp2/exp3/labels'\n",
    "ground_truths_dir = '/home/wgt/yolov7/test/labels'\n",
    "\n",
    "evaluate_model(predictions_dir, ground_truths_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _**Analysis of the results**_\n",
    "<br> Precision: _0.5411_. The precision increased slightly compared to the first test run. The model has improved in detecting positive cases.\n",
    "\n",
    "<br> Recall: _0.6790_. While the precision increased compared to the previous test run, the recall decrease more significantly. The increase in precision is desired, however, a large decrease in the recall may counteract the latter. This will be reflected in the F1 score of the model.\n",
    "\n",
    "<br> F1 Score: _0.6022_. This score is lower than the one we observed in the first training of the model. As mentioned, despite the precision increasing by around 1%, the decrease in recall was much larger, which led to this lower recall score. The results are not satisfactory, however, by observing TensorBoard we can see that after around 40 and 55 epochs, the precision and recall values stagnate respectively. Therefore, we can see that there is no need to have so many epochs especially if the metric values are not changing. \n",
    "\n",
    "<br> It is noteworthy to mention that precision and recall are inversely proprotional. However, it is still possible to improve both the values we got from the testing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
