{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LoadingBMWDataset import ObjectDetectionDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import albumentations as A\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_annotations_to_yolo_format(img_dir, annotation_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Ensure the output directory exists\n",
    "\n",
    "    # Iterating over each annotation file in the annotation directory, and convert the annotations to YOLO format\n",
    "    for annotation_file in os.listdir(annotation_dir):\n",
    "        if annotation_file.endswith('.json'):\n",
    "            json_path = os.path.join(annotation_dir, annotation_file)\n",
    "            with open(json_path) as f:\n",
    "                annotations = json.load(f)\n",
    "            \n",
    "            # Deriving the corresponding image file path and load it to get dimensions\n",
    "            img_file = annotation_file.replace('.json', '.jpg')\n",
    "            img_path = os.path.join(img_dir, img_file)\n",
    "            with Image.open(img_path) as img:\n",
    "                img_width, img_height = img.size\n",
    "            \n",
    "            # Converting annotations to YOLO format\n",
    "            yolo_annotations = []\n",
    "            for annot in annotations:\n",
    "                class_id = annot['ObjectClassId'] - 1  # Assuming class IDs start from 1, adjust if necessary\n",
    "                x_center = ((annot['Right'] + annot['Left']) / 2) / img_width\n",
    "                y_center = ((annot['Bottom'] + annot['Top']) / 2) / img_height\n",
    "                width = (annot['Right'] - annot['Left']) / img_width\n",
    "                height = (annot['Bottom'] - annot['Top']) / img_height\n",
    "                yolo_annotations.append(f\"{class_id} {x_center} {y_center} {width} {height}\")\n",
    "            \n",
    "            # Saving converted annotations to TXT file\n",
    "            txt_path = os.path.join(output_dir, annotation_file.replace('.json', '.txt'))\n",
    "            with open(txt_path, 'w') as f:\n",
    "                f.write('\\n'.join(yolo_annotations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = '/home/wgt/Desktop/InMind Academy/AI_Track/Amazing_Project/inmind_amazing_project/data/Training/images'\n",
    "annotation_dir = '/home/wgt/Desktop/InMind Academy/AI_Track/Amazing_Project/inmind_amazing_project/data/Training/labels/json'\n",
    "output_dir = '/home/wgt/Desktop/InMind Academy/AI_Track/Amazing_Project/inmind_amazing_project/data/Training/labels/yolo'\n",
    "\n",
    "convert_annotations_to_yolo_format(img_dir, annotation_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "\n",
    "annotations_txt_dir = '/home/wgt/Desktop/InMind Academy/AI_Track/Amazing_Project/inmind_amazing_project/data/Training/labels/yolo'\n",
    "img_dir = '/home/wgt/Desktop/InMind Academy/AI_Track/Amazing_Project/inmind_amazing_project/data/Training/images'\n",
    "\n",
    "# Listing all .txt annotation files\n",
    "annotation_files = glob.glob(os.path.join(annotations_txt_dir, '*.txt'))\n",
    "\n",
    "# Extracting corresponding image file names from annotation file names\n",
    "img_files = [os.path.join(img_dir, os.path.basename(f).replace('.txt', '.jpg')) for f in annotation_files]\n",
    "\n",
    "# Splitting the dataset into training and validation\n",
    "train_img_files, val_img_files, train_annotation_files, val_annotation_files = train_test_split(img_files, annotation_files, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 1995\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training images: {len(train_img_files)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving to YOLOV7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolov7_train_images_dir = '/home/wgt/yolov7/train/images'\n",
    "yolov7_train_labels_dir = '/home/wgt/yolov7/train/labels'\n",
    "\n",
    "os.makedirs(yolov7_train_images_dir, exist_ok=True)\n",
    "os.makedirs(yolov7_train_labels_dir, exist_ok=True)\n",
    "\n",
    "for img_file in train_img_files:\n",
    "    dest_file = os.path.join(yolov7_train_images_dir, os.path.basename(img_file))\n",
    "    shutil.copy(img_file, dest_file)\n",
    "    print(f\"Copied {img_file} to {dest_file}\")\n",
    "\n",
    "for label_file in train_annotation_files:\n",
    "    dest_file = os.path.join(yolov7_train_labels_dir, os.path.basename(label_file))\n",
    "    shutil.copy(label_file, dest_file)\n",
    "    print(f\"Copied {label_file} to {dest_file}\")\n",
    "\n",
    "yolov7_val_images_dir = '/home/wgt/yolov7/val/images'\n",
    "yolov7_val_labels_dir = '/home/wgt/yolov7/val/labels'\n",
    "\n",
    "os.makedirs(yolov7_val_images_dir, exist_ok=True)\n",
    "os.makedirs(yolov7_val_labels_dir, exist_ok=True)\n",
    "\n",
    "for img_file in val_img_files:\n",
    "    dest_file = os.path.join(yolov7_val_images_dir, os.path.basename(img_file))\n",
    "    shutil.copy(img_file, dest_file)\n",
    "    print(f\"Copied {img_file} to {dest_file}\")\n",
    "\n",
    "for label_file in val_annotation_files:\n",
    "    dest_file = os.path.join(yolov7_val_labels_dir, os.path.basename(label_file))\n",
    "    shutil.copy(label_file, dest_file)\n",
    "    print(f\"Copied {label_file} to {dest_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of the first Training process:\n",
    "\n",
    "<br>Class|||||Images||||Labels|||||||||||P||||||||||||R||||||||||mAP@.5|||mAP@.5:.95 </br>\n",
    "<br>  all||||||||||483|||||||||1995|||||||||||0.96|||||||0.954|||||0.976||||0.919 </br>\n",
    "<br>  dolly|||||||483|||||||||847||||||||||||0.934||||||0.913|||||0.957||||0.855 </br>\n",
    "<br>  bin|||||||||483|||||||||349||||||||||||0.961||||||0.983|||||0.992||||0.948 </br>\n",
    "<br>  jack||||||||483|||||||||799||||||||||||0.986||||||0.967|||||0.98|||||0.952 </br>\n",
    "\n",
    "<br> We can see that the precision is very high (96% overall), as well as the recall (95.4%). \n",
    "<br> The Mean Average Precision (AP) at IoU (Intersection over Union) threshold of 0.5 indicates a 97.6% meaning a very high accuracy.\n",
    "<br> The the mean AP calculated at different IoU thresholds from 0.5 to 0.95 (with a step size of 0.05), provides a more comprehensive view of the model performance across various strictness levels of object detection.\n",
    "\n",
    "<br> The model shows high precision and recall across the classes dolly, bin, and jack, with overall great mAP scores, which indicates strong performance in both recognizing the correct objects (high recall) and ensuring that most detections are accurate (high precision)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
